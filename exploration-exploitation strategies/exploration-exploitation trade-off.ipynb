{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q learning with different exploration strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "gym version == \u001B[1;36m0.26\u001B[0m.\u001B[1;36m2\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">gym version == <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.26</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from enum import Enum\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "from gym import register\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from rich.console import Console\n",
    "file_dir_name = os.path.dirname(os.getcwd())\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "if file_dir_name not in sys.path:\n",
    "    sys.path.append(file_dir_name)\n",
    "\n",
    "from gym.envs.toy_text import FrozenLakeEnv\n",
    "from simple_grid import DrunkenWalkEnv\n",
    "\n",
    "# 可以将print输出美化\n",
    "cs = Console()\n",
    "warnings.filterwarnings('ignore')\n",
    "cs.print(f\"gym version == {gym.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &#x1F680; 1、定义算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning(object):\n",
    "    def __init__(self,cfg):\n",
    "        self.explore_type = cfg.explore_type # 探索策略类型\n",
    "        self.n_actions = cfg.n_actions \n",
    "        self.lr = cfg.lr  # 学习率\n",
    "        self.gamma = cfg.gamma  \n",
    "        self.epsilon = cfg.epsilon_start\n",
    "        self.sample_count = 0  \n",
    "        self.epsilon_start = cfg.epsilon_start\n",
    "        self.epsilon_end = cfg.epsilon_end\n",
    "        self.epsilon_decay = cfg.epsilon_decay\n",
    "        self.epsilon_decay_flag = cfg.epsilon_decay_flag\n",
    "        self.Q_table  = np.zeros((cfg.n_states, self.n_actions)) # 用np.array 作为QTable 第一维为state， 第二维维每个state可以采取的action\n",
    "        self.__ucb_init()\n",
    "    \n",
    "    @staticmethod\n",
    "    def explore_type_space():\n",
    "        return { 'epsilon_greedy', 'boltzmann', 'ucb', 'special_ucb', 'softmax', 'thompson'}\n",
    "    \n",
    "    def __softmax(self, actions_v):\n",
    "        return np.exp(actions_v + 1e-3 ) / np.sum(np.exp(actions_v + 1e-3), axis=0)\n",
    "\n",
    "    def __softmax_policy_init(self):\n",
    "        self.Q_table = np.random.random(self.Q_table.shape)\n",
    "\n",
    "    def __ucb_init(self):\n",
    "        self.ucb_sa_visit_cnt_arr = np.array([])\n",
    "        self.ucb_cnt = 0\n",
    "        self.ucb_print = 0\n",
    "        self.ucb_sa_visit_cnt_arr = np.ones(self.Q_table.shape)\n",
    "\n",
    "    def _sp_ucb_policy(self, s):\n",
    "        \"\"\"ucb策略\n",
    "        reference: \n",
    "        \"\"\"\n",
    "        self.ucb_cnt += 1\n",
    "        # 先验action\n",
    "        not_state_once = np.sum(self.ucb_sa_visit_cnt_arr > 1, axis=1).sum() < self.ucb_sa_visit_cnt_arr.shape[0]\n",
    "        if not_state_once:\n",
    "            a_final = self._e_greedy(s)\n",
    "            self.ucb_sa_visit_cnt_arr[s, a_final] += 1\n",
    "            return a_final\n",
    "\n",
    "        self.ucb_print += 1\n",
    "        if self.ucb_print == 1:\n",
    "            print(f'UCB-Start {self.ucb_cnt}')\n",
    "        b_t = self.__softmax(self.Q_table[s]) + self.__softmax(np.sqrt(2 * np.log(self.ucb_cnt) / self.ucb_sa_visit_cnt_arr[s]))\n",
    "        a_final = np.argmax(b_t)\n",
    "        self.ucb_sa_visit_cnt_arr[s, a_final] += 1\n",
    "        return a_final\n",
    "    \n",
    "    def _e_greedy(self, state):\n",
    "        self.epsilon = self.epsilon_end\n",
    "        if self.epsilon_decay_flag:\n",
    "            self.epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \\\n",
    "                math.exp(-1. * self.sample_count / self.epsilon_decay) # epsilon是会递减的，这里选择指数递减\n",
    "        # e-greedy 策略\n",
    "        if np.random.uniform(0, 1) < self.epsilon: # 随机选择动作\n",
    "            return np.random.randint(len(self.Q_table[state]))\n",
    "        if sum(self.Q_table[state]) != 0:\n",
    "            return np.argmax(self.Q_table[state]) # 选择Q(s,a)最大对应的动作\n",
    "        return np.random.randint(len(self.Q_table[state]))\n",
    "        \n",
    "    def sample_action(self, state):\n",
    "        ''' 采样动作，训练时用\n",
    "        '''\n",
    "        self.sample_count += 1\n",
    "        if self.explore_type == 'epsilon_greedy':\n",
    "            return self._e_greedy(state)\n",
    "        elif self.explore_type == 'boltzmann':\n",
    "            # boltzmann 策略\n",
    "            action_probs = np.exp(self.Q_table[int(state)] / self.epsilon) / np.sum(np.exp(self.Q_table[int(state)] / self.epsilon))\n",
    "            action = np.random.choice(self.n_actions, p=action_probs)\n",
    "            return action\n",
    "        elif self.explore_type == 'ucb':\n",
    "            # ucb 策略\n",
    "            if self.sample_count < self.n_actions:\n",
    "                action = self.sample_count\n",
    "            else:\n",
    "                action = np.argmax(\n",
    "                    self.__softmax(self.Q_table[int(state)]) + \n",
    "                    self.__softmax(self.epsilon * np.sqrt(np.log(self.sample_count) / self.sample_count))\n",
    "                )\n",
    "            return action\n",
    "        elif self.explore_type == 'special_ucb':\n",
    "            return self._sp_ucb_policy(state)\n",
    "        elif self.explore_type == 'softmax':\n",
    "            if self.sample_count <= 1:\n",
    "                self.__softmax_policy_init()\n",
    "            # softmax 策略\n",
    "            action_probs = np.exp(self.Q_table[int(state)] / self.epsilon) / np.sum(np.exp(self.Q_table[int(state)] / self.epsilon))\n",
    "            action = np.random.choice(self.n_actions, p=action_probs)\n",
    "            return action\n",
    "        elif self.explore_type == 'thompson':\n",
    "            # thompson 策略\n",
    "            success_p = self.__softmax(self.Q_table[state])\n",
    "            failed_p = 1 - success_p\n",
    "            samples = np.random.beta(success_p, failed_p)\n",
    "            return np.argmax(samples)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def predict_action(self,state):\n",
    "        ''' 预测或选择动作，测试时用\n",
    "        '''\n",
    "        if self.explore_type == 'epsilon_greedy':\n",
    "            action = np.argmax(self.Q_table[int(state)])\n",
    "            return action\n",
    "        elif self.explore_type == 'boltzmann':\n",
    "            action_probs = np.exp(self.Q_table[int(state)] / self.epsilon) / np.sum(np.exp(self.Q_table[int(state)] / self.epsilon))\n",
    "            action = np.random.choice(self.n_actions, p=action_probs)\n",
    "            return action\n",
    "        elif self.explore_type in ['ucb', 'special_ucb']:\n",
    "            action = np.argmax(self.Q_table[int(state)])\n",
    "            return action\n",
    "        elif self.explore_type == 'softmax':\n",
    "            action_probs = np.exp(self.Q_table[int(state)] / self.epsilon) / np.sum(np.exp(self.Q_table[int(state)] / self.epsilon))\n",
    "            action = np.random.choice(self.n_actions, p=action_probs)\n",
    "            return action\n",
    "        elif self.explore_type == 'thompson':\n",
    "            success_p = self.__softmax(self.Q_table[state])\n",
    "            failed_p = 1 - success_p\n",
    "            samples = np.random.beta(success_p, failed_p)\n",
    "            return np.argmax(samples)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def update(self, state, action, reward, next_state, terminated):\n",
    "        Q_predict = self.Q_table[int(state)][action] \n",
    "        if terminated: # 终止状态\n",
    "            Q_target = reward  \n",
    "        else:\n",
    "            Q_target = reward + self.gamma * np.max(self.Q_table[int(next_state)]) \n",
    "        self.Q_table[int(state)][action] += self.lr * (Q_target - Q_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、定义训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_seed(env, seed = 1):\n",
    "    ''' omnipotent seed for RL, attention the position of seed function, you'd better put it just following the env create function\n",
    "    Args:\n",
    "        env (_type_): \n",
    "        seed (int, optional): _description_. Defaults to 1.\n",
    "    '''\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    import random\n",
    "    # print(f\"seed = {seed}\")\n",
    "    env.seed(seed) # env config\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed) # config for CPU\n",
    "    torch.cuda.manual_seed(seed) # config for GPU\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed) # config for python scripts\n",
    "    # config for cudnn\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    print(f'Set env random_seed = {seed}')\n",
    "\n",
    "\n",
    "def train(cfg, env, agent):\n",
    "    out_dict = {'algo': cfg.algo_name, 'explore_type': cfg.explore_type}\n",
    "    best = 1 if cfg.env_name == 'FrozenLakeEasy-v0' else 10\n",
    "    def conv_finished(rewards):\n",
    "        if len(rewards) <= 11:\n",
    "            return len(rewards), False\n",
    "        if np.mean(rewards[-11:-1]) == best:\n",
    "            return len(rewards), np.mean(rewards[-11:-1]) == np.mean(rewards[-10:])\n",
    "        return len(rewards), False\n",
    "\n",
    "    print(f'环境:{cfg.env_name}, 算法:{cfg.algo_name}, 设备:{cfg.device}')\n",
    "    rewards = []  # 记录奖励\n",
    "    fi_flag = False\n",
    "    tq_bar = tqdm(range(cfg.train_eps))\n",
    "    all_seed(env, seed = cfg.seed)\n",
    "    for i_ep in tq_bar:\n",
    "        tq_bar.set_description(f'Tarin[ {i_ep + 1}/{cfg.train_eps} ](Epsilon：{agent.epsilon:.3f})')\n",
    "        ep_reward = 0  # 记录每个回合的奖励\n",
    "        ep_step = 0\n",
    "        # 重置环境,即开始新的回合\n",
    "        state = env.reset()  \n",
    "        while cfg.max_step > ep_step:\n",
    "            ep_step += 1\n",
    "            action = agent.sample_action(state)  # 根据算法采样一个动作\n",
    "            next_state, reward, terminated, info = env.step(action)  # 与环境进行一次动作交互\n",
    "            agent.update(state, action, reward, next_state, terminated)  # Q学习算法更新\n",
    "            state = next_state  # 更新状态\n",
    "            ep_reward += reward\n",
    "            if terminated:\n",
    "                break\n",
    "        rewards.append(ep_reward)\n",
    "        last_10_mean = np.mean(rewards[-10:])\n",
    "\n",
    "        if not fi_flag:\n",
    "            step, fi_flag = conv_finished(rewards)\n",
    "            out_dict['conv_eps'] = step\n",
    "\n",
    "        tq_bar.set_postfix(reward=f'{last_10_mean:.3f}')\n",
    "        tq_bar.update()\n",
    "\n",
    "    out_dict['rewards'] = rewards\n",
    "    return out_dict  #TODO:可以加收敛的回合数\n",
    "\n",
    "\n",
    "def test(cfg, env, agent):\n",
    "    out_dict = {'algo': cfg.algo_name, 'explore_type': cfg.explore_type}\n",
    "    print(f'环境：{cfg.env_name}, 算法：{cfg.algo_name}, 设备：{cfg.device}')\n",
    "    rewards = []  # 记录所有回合的奖励\n",
    "    steps = []\n",
    "    tq_bar = tqdm(range(cfg.test_eps))\n",
    "    for i_ep in tq_bar:\n",
    "        tq_bar.set_description(f'Test[ {i_ep + 1}/{cfg.test_eps} ](Epsilon：{agent.epsilon:.3f})')\n",
    "        ep_step = 0 # 记录每个episode的智能体行走的次数\n",
    "        ep_reward = 0  # 记录每个episode的reward\n",
    "        env.seed(cfg.seed)\n",
    "        state = env.reset()  # 重置环境, 重新开一局（即开始新的一个回合）\n",
    "        while cfg.max_step > ep_step:\n",
    "            action = agent.predict_action(state)  # 根据算法选择一个动作\n",
    "            next_state, reward, terminated, info = env.step(action)  # 与环境进行一个交互\n",
    "            state = next_state  # 更新状态\n",
    "            ep_reward += reward\n",
    "            ep_step += 1\n",
    "            if terminated:\n",
    "                break\n",
    "        rewards.append(ep_reward)\n",
    "        steps.append(ep_step)\n",
    "        last_10_mean = np.mean(rewards[-10:])\n",
    "        \n",
    "        tq_bar.set_postfix(reward=f'{last_10_mean:.3f}')\n",
    "        tq_bar.update()\n",
    "\n",
    "    out_dict['rewards'] = rewards\n",
    "    out_dict['steps'] = steps\n",
    "    out_dict['meanReward'] = np.mean(rewards)\n",
    "    out_dict['meanSteps'] = np.mean(steps)\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \t&#x1F4CA; 3、一些可视化的工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Direct(Enum):\n",
    "    Left_v = 0\n",
    "    Down_v = 1\n",
    "    Right_v = 2\n",
    "    Up_v = 3\n",
    "    Left = -1\n",
    "    Down = -1\n",
    "    Right = +1\n",
    "    Up = +1\n",
    "    Left_s = r'$\\Leftarrow$'\n",
    "    Down_s = r'$\\Downarrow$'\n",
    "    Right_s = r'$\\Rightarrow$'\n",
    "    Up_s = r'$\\Uparrow$'\n",
    "\n",
    "\n",
    "class QTablePlot:\n",
    "    def __init__(self, Q_table, env):\n",
    "        self.Q_table = Q_table\n",
    "        self.env = env\n",
    "        self.env_rows, self.env_cols = env.nrow, env.ncol\n",
    "        self.table = np.zeros((env.nrow * 3, env.ncol * 3))\n",
    "        self.direct = Direct\n",
    "        self.text_record_dict = {}\n",
    "        self.record_SHG = {}\n",
    "\n",
    "    def _fill_table(self, q_table: np.ndarray):\n",
    "        text_str_list = [\n",
    "            Direct.Left_s.value, \n",
    "            Direct.Down_s.value, \n",
    "            Direct.Right_s.value, \n",
    "            Direct.Up_s.value\n",
    "        ]\n",
    "        env_desc_str = ''.join(''.join(i) for i in self.env.desc.astype(str))\n",
    "        for r in range(self.env_rows):\n",
    "            for c in range(self.env_cols):\n",
    "                s = r * self.env_cols + c\n",
    "                center_r = 1 + (self.env_rows - r - 1) * 3\n",
    "                # center_r = 1 + r * 3\n",
    "                center_c = 1 + c * 3\n",
    "                # \"Left\",\"Down\",\"Right\",\"Up\"\n",
    "                self.table[\n",
    "                    center_r , center_c + Direct.Left.value\n",
    "                ] = q_table[s, Direct.Left_v.value]\n",
    "                self.table[\n",
    "                    center_r , center_c + Direct.Right.value\n",
    "                ] = q_table[s, Direct.Right_v.value]\n",
    "                self.table[\n",
    "                    center_r + Direct.Down.value , center_c\n",
    "                ] = q_table[s, Direct.Down_v.value]\n",
    "                self.table[\n",
    "                    center_r + Direct.Up.value , center_c\n",
    "                ] = q_table[s, Direct.Up_v.value]\n",
    "                # center\n",
    "                self.table[\n",
    "                    center_r , center_c\n",
    "                ] = q_table[s].mean()\n",
    "\n",
    "                idx = np.argmax(q_table[s])\n",
    "                name = text_str_list[idx]\n",
    "                mv_v = Direct[Direct(idx).name[:-2]].value\n",
    "\n",
    "                if idx in [0, 2]:\n",
    "                    self.text_record_dict[f'{center_r},{center_c + mv_v}'] = name\n",
    "\n",
    "                else:\n",
    "                    self.text_record_dict[f'{center_r + mv_v},{center_c}'] = name\n",
    "                \n",
    "                if env_desc_str[s] != '.' and env_desc_str[s] != 'F':\n",
    "                    self.record_SHG[f'{center_r},{center_c}'] = env_desc_str[s]\n",
    "    \n",
    "    @staticmethod\n",
    "    def exp_smooth(data, weight=0.9):  \n",
    "        '''用于指数平滑曲线\n",
    "        '''\n",
    "        last = data[0]  # First value in the plot (first timestep)\n",
    "        smoothed = list()\n",
    "        for point in data:\n",
    "            smoothed_val = last * weight + (1 - weight) * point  # 计算平滑值\n",
    "            smoothed.append(smoothed_val)                    \n",
    "            last = smoothed_val                                \n",
    "        return smoothed\n",
    "\n",
    "    def exp_smooth_plot_rewards(self, rewards, title=\"\", ax=None):\n",
    "        sns.set()\n",
    "        if ax:\n",
    "            ax.set_title(f\"{title}learning curve(expSmooth)\")\n",
    "            ax.set_xlim(0, len(rewards), 10)  # 设置x轴的范围\n",
    "            ax.set_xlabel('epsiodes')\n",
    "            ax.plot(rewards, label='rewards')\n",
    "            ax.plot(self.exp_smooth(rewards), label='smoothed')\n",
    "            ax.legend()\n",
    "            return\n",
    "        sns.set()\n",
    "        plt.figure()  # 创建一个图形实例，方便同时多画几个图\n",
    "        plt.title(f\"{title}\")\n",
    "        plt.xlim(0, len(rewards), 10)  # 设置x轴的范围\n",
    "        plt.xlabel('epsiodes')\n",
    "        plt.plot(rewards, label='rewards')\n",
    "        plt.plot(self.exp_smooth(rewards), label='smoothed')\n",
    "        plt.legend()\n",
    "\n",
    "    def rewards_plot(self, rewards, window_size=10, freq=1, title='', axes=None):\n",
    "        record_arr = np.array(rewards)\n",
    "        record_smooth = []\n",
    "        std_list = []\n",
    "        if len(record_arr) < window_size:\n",
    "            window_size = 10\n",
    "        for i in range(1, len(rewards), freq):\n",
    "            tmp_arr = record_arr[max(0, i-window_size):i]\n",
    "            record_smooth.append(np.mean(tmp_arr))\n",
    "            std_list.append(np.std(tmp_arr))\n",
    "        \n",
    "        if axes is None:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(16, 4))\n",
    "\n",
    "        self.exp_smooth_plot_rewards(rewards, title, ax=axes[0])        \n",
    "        axes[1].set_title(f'{title}Learning Rewards Trend')\n",
    "        axes[1].plot(record_smooth, label=f'Rewards for each {window_size} episode.')\n",
    "        axes[1].fill_between(\n",
    "            x=np.arange(len(record_smooth)),\n",
    "            y1=np.array(record_smooth) - np.array(std_list), \n",
    "            y2=np.array(record_smooth) + np.array(std_list), \n",
    "            color='green', alpha=0.1\n",
    "            )\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def Qtable_plot(self, title=''):\n",
    "        self._fill_table(self.Q_table)\n",
    "        plt.title(f'{title}Agent Qtable')\n",
    "        plt.imshow(\n",
    "            self.table,\n",
    "            cmap=\"RdBu_r\", \n",
    "            interpolation=\"bilinear\",\n",
    "            vmin=-np.abs(self.table).max() , \n",
    "            vmax=np.abs(self.table).max()\n",
    "        )\n",
    "        plt.xlim(-0.5, 3 * self.env_cols - 0.5)\n",
    "        plt.ylim(-0.5, 3 * self.env_rows - 0.5)\n",
    "        plt.xticks(np.arange(-0.5, 3 * self.env_cols, 3), range(self.env_cols + 1))\n",
    "        plt.yticks(np.arange(-0.5, 3 * self.env_rows, 3), range(self.env_rows + 1))\n",
    "        plt.grid(True)\n",
    "        for k, v in self.text_record_dict.items():\n",
    "            y_, x_ = k.split(',')\n",
    "            plt.text(int(x_), int(y_), f'{v}', va='center', ha='center')\n",
    "        \n",
    "        for k, v in self.record_SHG.items():\n",
    "            y_, x_ = k.split(',')\n",
    "            plt.text(int(x_), int(y_), f'{v}', va='center', ha='center', color='darkred', fontdict={'weight': 'bold'})\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、定义环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.random._generator.Generator' object has no attribute 'rand'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m walk_in_the_park_env \u001B[38;5;241m=\u001B[39m \u001B[43mDrunkenWalkEnv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmap_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mwalkInThePark\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m alley_env \u001B[38;5;241m=\u001B[39m DrunkenWalkEnv(map_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtheAlley\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      4\u001B[0m lake_env \u001B[38;5;241m=\u001B[39m FrozenLakeEnv(is_slippery\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32mD:\\repository\\reinforcement-learning\\exploration-exploitation strategies\\simple_grid.py:277\u001B[0m, in \u001B[0;36mDrunkenWalkEnv.__init__\u001B[1;34m(self, desc, map_name, is_slippery)\u001B[0m\n\u001B[0;32m    274\u001B[0m                 \u001B[38;5;28;01mfor\u001B[39;00m b \u001B[38;5;129;01min\u001B[39;00m [(a\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m%\u001B[39m\u001B[38;5;241m4\u001B[39m, (a\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m%\u001B[39m\u001B[38;5;241m4\u001B[39m]:\n\u001B[0;32m    275\u001B[0m                     construct_transition_for_intended(row, col, b, \u001B[38;5;241m0.1\u001B[39m, li)\n\u001B[1;32m--> 277\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mDrunkenWalkEnv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mnS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnA\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mP\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43misd\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\repository\\reinforcement-learning\\exploration-exploitation strategies\\simple_grid.py:56\u001B[0m, in \u001B[0;36mDiscreteEnv.__init__\u001B[1;34m(self, nS, nA, P, isd)\u001B[0m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservation_space \u001B[38;5;241m=\u001B[39m spaces\u001B[38;5;241m.\u001B[39mDiscrete(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnS)\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mseed()\n\u001B[1;32m---> 56\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39ms \u001B[38;5;241m=\u001B[39m \u001B[43mcategorical_sample\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43misd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnp_random\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\repository\\reinforcement-learning\\exploration-exploitation strategies\\simple_grid.py:26\u001B[0m, in \u001B[0;36mcategorical_sample\u001B[1;34m(prob_n, np_random)\u001B[0m\n\u001B[0;32m     24\u001B[0m prob_n \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(prob_n)\n\u001B[0;32m     25\u001B[0m csprob_n \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mcumsum(prob_n)\n\u001B[1;32m---> 26\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m (csprob_n \u001B[38;5;241m>\u001B[39m \u001B[43mnp_random\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrand\u001B[49m())\u001B[38;5;241m.\u001B[39margmax()\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'numpy.random._generator.Generator' object has no attribute 'rand'"
     ]
    }
   ],
   "source": [
    "walk_in_the_park_env = DrunkenWalkEnv(map_name='walkInThePark')\n",
    "alley_env = DrunkenWalkEnv(map_name='theAlley')\n",
    "\n",
    "lake_env = FrozenLakeEnv(is_slippery=False)\n",
    "register(id='FrozenLakeEasy-v0', entry_point=\"gym.envs.toy_text:FrozenLakeEnv\", kwargs={\"is_slippery\": False})\n",
    "gym_env = gym.make('FrozenLakeEasy-v0')\n",
    "\n",
    "env_dict = {\n",
    "    'theAlley': alley_env ,\n",
    "    'walkInThePark': walk_in_the_park_env,\n",
    "    'FrozenLakeEasy-v0': gym_env,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 因为直接is_slippery=False 无法关闭输出，所以用register的方式关闭 滑倒的输出\n",
    "state = lake_env.reset()\n",
    "st = gym_env.reset()\n",
    "lake_env.step(state), gym_env.step(st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &#x1F4CC; 5、设置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Config:\n",
    "    '''配置参数\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.env_name = 'theAlley' # 环境名称\n",
    "        self.algo_name = 'Q-Learning' # 算法名称\n",
    "        self.explore_type = 'epsilon_greedy' # 探索策略\n",
    "        self.train_eps = 400 # 训练回合数\n",
    "        self.test_eps = 20 # 测试回合数\n",
    "        self.max_steps = 200 # 每个回合最大步数\n",
    "        self.epsilon_start = 0.65 #  e-greedy策略中epsilon的初始值\n",
    "        self.epsilon_end = 0.005 #  e-greedy策略中epsilon的最终值\n",
    "        self.epsilon_decay = 100 #  e-greedy策略中epsilon的衰减率\n",
    "        self.epsilon_decay_flag = True # e-greedy策略中epsilon是否衰减\n",
    "        self.max_step = 500 # 一盘游戏如果智能体行动超过 max_step 次就终止\n",
    "        self.gamma = 0.9 # 折扣因子\n",
    "        self.lr = 0.1 # 学习率\n",
    "        self.seed = 1 # 随机种子\n",
    "        self.device = torch.device('cpu')\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f'Config(Env={self.env_name}, Algo={self.algo_name}, exploreType={self.explore_type}(gamma={self.gamma},learning_rate={self.lr}))'\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "\n",
    "\n",
    "final_res = [] # 用于记录每个环境的最终结果\n",
    "cfg = Config() \n",
    "cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &#x2728;5、探索策略研究"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs.print( 'QLearning.explore_type_space():\\n', QLearning.explore_type_space() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary_dict = defaultdict(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1、softmax 策略探索\n",
    "\n",
    "softmax探索的时候, 对Qtable中所有值进行了随机初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cfg.explore_type = 'softmax'\n",
    "\n",
    "for env_name, env in env_dict.items():\n",
    "    print('---'*45)\n",
    "    print(f'EnvName = {env_name}')\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    print(f'状态数：{n_states}, 动作数：{n_actions}')\n",
    "    setattr(cfg, 'env_name', env_name)\n",
    "    setattr(cfg, 'n_states', n_states)\n",
    "    setattr(cfg, 'n_actions', n_actions)\n",
    "    cs.print(cfg)\n",
    "    agent = QLearning(cfg)\n",
    "    train_res = train(cfg, env, agent) # TODO：这里输出结果用字典表示，方便后面提取数据\n",
    "    final_play_res = test(cfg, env, agent)\n",
    "    # summary\n",
    "    summary_dict['envName'].append(env_name)\n",
    "    summary_dict['algoName'].append(train_res['algo'])\n",
    "    summary_dict['exploreType'].append(train_res['explore_type'])\n",
    "    summary_dict['convEps'].append(train_res['conv_eps'])\n",
    "    summary_dict['meanReward'].append(final_play_res['meanReward'])\n",
    "    summary_dict['meanSteps'].append(final_play_res['meanSteps'])\n",
    "    print('convEps:', summary_dict['convEps'][-1])\n",
    "    # Plot\n",
    "    PLOTER = QTablePlot(agent.Q_table, env)\n",
    "    PLOTER.rewards_plot(train_res['rewards'], title=f'exploreType: {cfg.explore_type} | ')\n",
    "    PLOTER.Qtable_plot(f'exploreType: {cfg.explore_type} | ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2、epsilon_greedy 策略探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cfg.explore_type = 'epsilon_greedy'\n",
    "\n",
    "for env_name, env in env_dict.items():\n",
    "    print('---'*45)\n",
    "    print(f'EnvName = {env_name}')\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    print(f'状态数：{n_states}, 动作数：{n_actions}')\n",
    "    setattr(cfg, 'env_name', env_name)\n",
    "    setattr(cfg, 'n_states', n_states)\n",
    "    setattr(cfg, 'n_actions', n_actions)\n",
    "    cs.print(cfg)\n",
    "    agent = QLearning(cfg)\n",
    "    train_res = train(cfg, env, agent) # TODO：这里输出结果用字典表示，方便后面提取数据\n",
    "    final_play_res = test(cfg, env, agent)\n",
    "    # summary\n",
    "    summary_dict['envName'].append(env_name)\n",
    "    summary_dict['algoName'].append(train_res['algo'])\n",
    "    summary_dict['exploreType'].append(train_res['explore_type'])\n",
    "    summary_dict['convEps'].append(train_res['conv_eps'])\n",
    "    summary_dict['meanReward'].append(final_play_res['meanReward'])\n",
    "    summary_dict['meanSteps'].append(final_play_res['meanSteps'])\n",
    "    print('convEps:', summary_dict['convEps'][-1])\n",
    "    # Plot\n",
    "    PLOTER = QTablePlot(agent.Q_table, env)\n",
    "    PLOTER.rewards_plot(train_res['rewards'], title=f'exploreType: {cfg.explore_type} | ')\n",
    "    PLOTER.Qtable_plot(f'exploreType: {cfg.explore_type} | ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3、boltzmann 策略探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cfg.explore_type = 'boltzmann'\n",
    "\n",
    "for env_name, env in env_dict.items():\n",
    "    print('---'*45)\n",
    "    print(f'EnvName = {env_name}')\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    print(f'状态数：{n_states}, 动作数：{n_actions}')\n",
    "    setattr(cfg, 'env_name', env_name)\n",
    "    setattr(cfg, 'n_states', n_states)\n",
    "    setattr(cfg, 'n_actions', n_actions)\n",
    "    cs.print(cfg)\n",
    "    agent = QLearning(cfg)\n",
    "    train_res = train(cfg, env, agent) # TODO：这里输出结果用字典表示，方便后面提取数据\n",
    "    final_play_res = test(cfg, env, agent)\n",
    "    # summary\n",
    "    summary_dict['envName'].append(env_name)\n",
    "    summary_dict['algoName'].append(train_res['algo'])\n",
    "    summary_dict['exploreType'].append(train_res['explore_type'])\n",
    "    summary_dict['convEps'].append(train_res['conv_eps'])\n",
    "    summary_dict['meanReward'].append(final_play_res['meanReward'])\n",
    "    summary_dict['meanSteps'].append(final_play_res['meanSteps'])\n",
    "    print('convEps:', summary_dict['convEps'][-1])\n",
    "    # Plot\n",
    "    PLOTER = QTablePlot(agent.Q_table, env)\n",
    "    PLOTER.rewards_plot(train_res['rewards'], title=f'exploreType: {cfg.explore_type} | ')\n",
    "    PLOTER.Qtable_plot(f'exploreType: {cfg.explore_type} | ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4、thompson 策略探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cfg.explore_type = 'thompson'\n",
    "\n",
    "\n",
    "for env_name, env in env_dict.items():\n",
    "    print('---'*45)\n",
    "    print(f'EnvName = {env_name}')\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    print(f'状态数：{n_states}, 动作数：{n_actions}')\n",
    "    setattr(cfg, 'env_name', env_name)\n",
    "    setattr(cfg, 'n_states', n_states)\n",
    "    setattr(cfg, 'n_actions', n_actions)\n",
    "    cs.print(cfg)\n",
    "    agent = QLearning(cfg)\n",
    "    train_res = train(cfg, env, agent) # TODO：这里输出结果用字典表示，方便后面提取数据\n",
    "    final_play_res = test(cfg, env, agent)\n",
    "    # summary\n",
    "    summary_dict['envName'].append(env_name)\n",
    "    summary_dict['algoName'].append(train_res['algo'])\n",
    "    summary_dict['exploreType'].append(train_res['explore_type'])\n",
    "    summary_dict['convEps'].append(train_res['conv_eps'])\n",
    "    summary_dict['meanReward'].append(final_play_res['meanReward'])\n",
    "    summary_dict['meanSteps'].append(final_play_res['meanSteps'])\n",
    "    print('convEps:', summary_dict['convEps'][-1])\n",
    "    # Plot\n",
    "    PLOTER = QTablePlot(agent.Q_table, env)\n",
    "    PLOTER.rewards_plot(train_res['rewards'], title=f'exploreType: {cfg.explore_type} | ')\n",
    "    PLOTER.Qtable_plot(f'exploreType: {cfg.explore_type} | ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5、ucb 策略探索\n",
    "\n",
    "special_ucb 在前期用e-greedy进行了初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cfg.explore_type = 'ucb'\n",
    "\n",
    "for env_name, env in env_dict.items():\n",
    "    print('---'*45)\n",
    "    print(f'EnvName = {env_name}')\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    print(f'状态数：{n_states}, 动作数：{n_actions}')\n",
    "    setattr(cfg, 'env_name', env_name)\n",
    "    setattr(cfg, 'n_states', n_states)\n",
    "    setattr(cfg, 'n_actions', n_actions)\n",
    "    cs.print(cfg)\n",
    "    agent = QLearning(cfg)\n",
    "    train_res = train(cfg, env, agent) # TODO：这里输出结果用字典表示，方便后面提取数据\n",
    "    final_play_res = test(cfg, env, agent)\n",
    "    # summary\n",
    "    summary_dict['envName'].append(env_name)\n",
    "    summary_dict['algoName'].append(train_res['algo'])\n",
    "    summary_dict['exploreType'].append(train_res['explore_type'])\n",
    "    summary_dict['convEps'].append(train_res['conv_eps'])\n",
    "    summary_dict['meanReward'].append(final_play_res['meanReward'])\n",
    "    summary_dict['meanSteps'].append(final_play_res['meanSteps'])\n",
    "    print('convEps:', summary_dict['convEps'][-1])\n",
    "    # Plot\n",
    "    PLOTER = QTablePlot(agent.Q_table, env)\n",
    "    PLOTER.rewards_plot(train_res['rewards'], title=f'exploreType: {cfg.explore_type} | ')\n",
    "    PLOTER.Qtable_plot(f'exploreType: {cfg.explore_type} | ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.explore_type = 'special_ucb'\n",
    "\n",
    "for env_name, env in env_dict.items():\n",
    "    print('---'*45)\n",
    "    print(f'EnvName = {env_name}')\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    print(f'状态数：{n_states}, 动作数：{n_actions}')\n",
    "    setattr(cfg, 'env_name', env_name)\n",
    "    setattr(cfg, 'n_states', n_states)\n",
    "    setattr(cfg, 'n_actions', n_actions)\n",
    "    cs.print(cfg)\n",
    "    agent = QLearning(cfg)\n",
    "    train_res = train(cfg, env, agent) # TODO：这里输出结果用字典表示，方便后面提取数据\n",
    "    final_play_res = test(cfg, env, agent)\n",
    "    # summary\n",
    "    summary_dict['envName'].append(env_name)\n",
    "    summary_dict['algoName'].append(train_res['algo'])\n",
    "    summary_dict['exploreType'].append(train_res['explore_type'])\n",
    "    summary_dict['convEps'].append(train_res['conv_eps'])\n",
    "    summary_dict['meanReward'].append(final_play_res['meanReward'])\n",
    "    summary_dict['meanSteps'].append(final_play_res['meanSteps'])\n",
    "    print('convEps:', summary_dict['convEps'][-1])\n",
    "    # Plot\n",
    "    PLOTER = QTablePlot(agent.Q_table, env)\n",
    "    PLOTER.rewards_plot(train_res['rewards'], title=f'exploreType: {cfg.explore_type} | ')\n",
    "    PLOTER.Qtable_plot(f'exploreType: {cfg.explore_type} | ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &#x2728; 6、总结\n",
    "<font color=darkred>\n",
    "\n",
    "- &#x1F4CD; 在action有限的情况下，一般epsilon_greedy探索策略更加的简单高效, 可以将epsilon_greedy探索策略作为第一选择策略    \n",
    "- &#x1F4CD; 在epsilon_greedy探索策略表现不佳的时候，我们可以第一优先使用softmax探索策略  \n",
    "</font>\n",
    "\n",
    "**1- `softmax`探索策略：**\n",
    "- 在`FrozenLakeEasy-v0` 环境中难以收敛\n",
    "    - 训练好的QTable几乎不可用：\n",
    "      - 在`FrozenLakeEasy-v0`: 进行20轮游戏，每局平均获取奖励0.10，每局行动8.35步\n",
    "- 在`theAlley` <font color=darkred>进行284轮次之后收敛</font>，用训练好的QTable进行20轮游戏，每局平均获取奖励4.75，每局行动42.10步。\n",
    "- 在`walkInThePark` <font color=darkred>进行114轮次之后收敛</font>，用训练好的QTable进行20轮游戏，每局平均获取奖励10，每局行动17.95步。\n",
    "\n",
    "**2- `epsilon_greedy`探索策略：**\n",
    "- 在`theAlley` 环境中难以收敛且训练的QTable不可用\n",
    "- 在`FrozenLakeEasy-v0` <font color=darkred>进行156轮次之后收敛</font>，用训练好的QTable进行20轮游戏，每局平均获取奖励1，每局行动6步。\n",
    "- 在`walkInThePark` <font color=darkred>进行94轮次之后收敛</font>，用训练好的QTable进行20轮游戏，每局平均获取奖励10，每局行动15步。\n",
    "\n",
    "\n",
    "**3- `boltzmann`探索策略：**\n",
    "- `theAlley` 和 `FrozenLakeEasy-v0` 环境中难以收敛\n",
    "    - 训练好的QTable有一定的可用性：\n",
    "      - 在`theAlley` ：进行20轮游戏，每局平均获取奖励3.25，每局行动39.05步\n",
    "      - 在`FrozenLakeEasy-v0` 中几乎不可用\n",
    "- 在`walkInThePark` <font color=darkred>进行143轮次之后收敛</font>，用训练好的QTable进行20轮游戏，每局平均获取奖励10，每局行动15.25步。\n",
    "\n",
    "\n",
    "\n",
    "**4- `thompson`探索策略：**\n",
    "- `theAlley` 和 `FrozenLakeEasy-v0` 环境中难以收敛\n",
    "    - 训练好的QTable有一定的可用性：\n",
    "      - 在`theAlley` ：进行20轮游戏，每局平均获取奖励6.25，每局行动43.55步\n",
    "      - 在`FrozenLakeEasy-v0` 中几乎不可用\n",
    "- 在`walkInThePark` <font color=darkred>进行74轮次之后收敛</font>，用训练好的QTable进行20轮游戏，每局平均获取奖励10，每局行动16.05步。\n",
    "\n",
    "**5-`ucb`探索策略：**\n",
    "- 在`theAlley`、 `FrozenLakeEasy-v0` 和 `walkInThePark` 环境中均难以收敛\n",
    "- 且训练的QTable不可用\n",
    "\n",
    "\n",
    "**5- `special_ucb`探索策略：**\n",
    "- 在`theAlley` 环境中难以收敛且训练的QTable不可用\n",
    "- 在`FrozenLakeEasy-v0` <font color=darkred>进行320轮次之后收敛</font>，用训练好的QTable进行20轮游戏，每局平均获取奖励1，每局行动6步。\n",
    "- 在`walkInThePark` <font color=darkred>进行67轮次之后收敛</font>，用训练好的QTable进行20轮游戏，每局平均获取奖励10，每局行动15步。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "display(pd.DataFrame(summary_dict).sort_values(by='exploreType'))\n",
    "pd.DataFrame(summary_dict).sort_values(by=['envName', 'meanReward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "8994a120d39b6e6a2ecc94b4007f5314b68aa69fc88a7f00edf21be39b41f49c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
